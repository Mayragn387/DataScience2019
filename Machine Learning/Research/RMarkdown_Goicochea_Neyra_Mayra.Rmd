---
title: "Machine Learning - Ejercicio Rmarkdown"
author: "Mayra Goicochea Neyra"
date: "7/11/2019"
output: 
  html_document:
    toc: true
    toc_float:
      collapsed: false
      smooth_scroll: false
    number_sections: true
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
if (!require("ggplot2")) install.packages("ggplot2")
if (!require("testthat")) install.packages("testthat")
library(testthat)
library(ggplot2)
library(readr)
```

# Introduction

Suppose that you are an administrator of a university and you want to know the chance of admission of each applicant based on their two exams. You have historical data from previous applicants which can be used as the training data for logistic regression.  Your task is to build a classification model that estimates each applicant’s probability of admission in university.

Now, we have understood classification problem that we are going to address. Let us understand the data. In data we have records of previous applicants’ two exams score and label whether applicant got admission or not (1 - if got admission 0 - otherwise).

# Assigment

## Test the *TestGradientDescent* function with the training set (*4_1_data.csv*). Obtain the confusion matrix.

First, I load the dataset from file *4_1_data.csv*.

```{r}
set.seed(777)
#Load data
data <- read.csv("../data/4_1_data.csv")

#Plot
cols <- c("0" = "red","1" = "green")
ggplot(data, aes(x = score.1, y = score.2, colour = as.factor(label))) +
  geom_point(size = 4, shape = 18,alpha = 0.6) +
  scale_colour_manual(values = cols,labels = c("Not admitted", "Admitted"),name = "Test Result") +
  theme_bw()
```

Then, I set predictor and response variables.

```{r}
#Predictor variables
X <- as.matrix(data[, c(1,2)])

#Add ones to X in the first column (matrix multiplication x b)
X <- cbind(rep(1, nrow(X)), X)

#Response variable
Y <- as.matrix(data$label)
```

So, I set the functions:

### Sigmoid Function

```{r}
Sigmoid <- function(x) { 
  1 / (1 + exp(-x))
}
```

### Cost Function

```{r}
# Ref: https://www.r-bloggers.com/logistic-regression-with-r-step-by-step-implementation-part-2/
# Cost Function
CostFunction <- function(parameters, X, Y) {
  n <- nrow(X)
  g <- Sigmoid(X %*% parameters)
  J <- (1/n) * sum((-Y * log(g)) - ((1 - Y) * log(1 - g)))
  return(J)
}
```

### TestGradientDescent 

```{r}
TestGradientDescent <- function(iterations = 1200, X, Y) {
  # Initialize (b, W)
  parameters <- rep(0, ncol(X))
  # Check evolution
  print(paste("Initial Cost Function value: ", 
              convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
  
  # updating (b, W) using gradient update
  
  # Derive theta using gradient descent using optim function
  # Look for information about the "optim" function (there are other options)
  parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y, 
                                   control = list(maxit = iterations))
  #set parameters
  parameters <- parameters_optimization$par
  # Check evolution
  print(paste("Final Cost Function value: ", 
              convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
  print(paste("Parameters value: ", 
              parameters))
 return(parameters) 
}
```

### Confusion Matrix

```{r}
ConfusionMatrix <- function(X, Y, parameters, cutoff = 0.5) {
  res.tbl <- NA  #First create a null table
 
 #Then full the table with the results of multiplication of each row with the parameters.
  for (i in 1:nrow(X)) {
      res <- Sigmoid(t(as.numeric(X[i,])) %*% parameters)
      res.tbl <- rbind(res.tbl, res)
   } 
  res.tbl <- res.tbl[-1,] # Delete first row because it has missing value
 
 #Then it creates the matrix validating if each result is greather than cutoff
  matrix <- table(Y, ifelse(res.tbl > cutoff, 1, 0),dnn = c("Actual","Predicted")) 
 #Accuracy, Precision, Recall & F1 score
 #First, I get the true values (true positive and true negative)
  true <- matrix[1,1] + matrix[2,2]
 #Accuracy = True values / Total cases
  print(paste("Accuracy: ", true / sum(matrix)))
 #Precision = TP / (TP+FP)
  print(paste("Precision: ", precision <- round(matrix[2,2]/sum(matrix[,2]),4)))
 #Recall = TP / (TP+FN)
  print(paste("Recall: ", recall <- round(matrix[2,2]/sum(matrix[2,]),4)))
 #F1-Score = 2*(Recall*Precision)/(Recall + Precision)
  print(paste("F1 score: ", round(2*(precision*recall)/(precision + recall),4)))
 return(matrix)
}
```

I use **testthat library** to execute the functions and help me to test them making sure everything is alright.

Then I run the functions to get the Confussion Matrix.

```{r}
test_that("Get Confusion Matrix with Optimal Parameters",{
parameters <- TestGradientDescent(X = X, Y = Y)
matrix <- ConfusionMatrix(X = X, Y = Y, parameters, cutoff = 0.68)
print(matrix)
})
```

## Obtain a graph representing how the cost function evolves depending of the number of iterations.

First I set a variable of max iterations of tests. Then, create a table of results.

```{r}
max_iterations <- 500
graph.table <- data.frame(cbind(seq(from = 1, to = max_iterations, by = 1), NA))
names(graph.table) <- c("Iteration", "Cost") 
```

With a for iteration, I get the optimal parameters of Cost Function and the values of Cost Function.

```{r}
parameters <- rep(x = 0, times = 3)
iterations <- 0
for (i in 1:max_iterations) {
  opt.parameters <- optim(par = parameters, fn = CostFunction, X = X, Y = Y, 
                                   control = list(maxit = iterations))
  iterations <- iterations + 1
  graph.table[i, 2] <- CostFunction(opt.parameters$par, X, Y)
}
```

Then, I make a graph of the results.

```{r}
ggplot(data = graph.table, aes(x = Iteration, y = Cost)) +
  geom_smooth(se = TRUE) +
  theme_bw()
```

## Explore other options using the *optim* function (see the methods section of the documentation). Explore other ways in R for estimating the Gradient Descent.

* Quasi-Newton Method (BFGS)

BFGS is an optimization method for multidimensional nonlinear unconstrained functions.
BFGS belongs to the family of quasi-Newton (Variable Metric) optimization methods that make use of both first-derivative (gradient) and second-derivative (Hessian matrix) based information of the function being optimized. More specifically, it is a quasi-Newton method which means that it approximates the second-order derivative rather than compute it directly. It is related to other quasi-Newton methods such as the DFP Method, Broyden’s method and the SR1 Method(@convexoptimizationinr).

```{r}
TestGradientDescentMethod <- function(iterations = 1200, X, Y, m="BFGS") {
  parameters <- rep(0, ncol(X))
  print(paste("Initial Cost Function value: ", 
              convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
  parameters_optimization <- optim(par = parameters, fn = CostFunction, X = X, Y = Y, 
                                   control = list(maxit = iterations), method = m)
  parameters <- parameters_optimization$par
  print(paste("Final Cost Function value: ", 
              convergence <- c(CostFunction(parameters, X, Y)), sep = ""))
 return(parameters) 
}
```


```{r}
test_that("Test Gradient Descent BFGS",{
parameters <- TestGradientDescentMethod(X = X, Y = Y, m = "BFGS")
matrix <- ConfusionMatrix(X = X, Y = Y, parameters, cutoff = 0.68)
print(matrix)
})
```

* CG Method

Method "CG" is a conjugate gradients method based on that by Fletcher and Reeves (1964) (but with the option of Polak–Ribiere or Beale–Sorenson updates). Conjugate gradient methods will generally be more fragile than the BFGS method, but as they do not store a matrix they may be successful in much larger optimization problems (@rdocumentation).

```{r}
test_that("Test Gradient Descent CG",{
parameters <- TestGradientDescentMethod(X = X, Y = Y, m = "CG")
matrix <- ConfusionMatrix(X = X, Y = Y, parameters, cutoff = 0.68)
print(matrix)
})
```

* SANN Method

Method "SANN" is by default a variant of simulated annealing given in Belisle (1992). Simulated-annealing belongs to the class of stochastic global optimization methods. It uses only function values but is relatively slow. It will also work for non-differentiable functions. This implementation uses the Metropolis function for the acceptance probability. By default the next candidate point is generated from a Gaussian Markov kernel with scale proportional to the actual temperature. If a function to generate a new candidate point is given, method "SANN" can also be used to solve combinatorial optimization problems(@rdocumentation).

```{r}
test_that("Test Gradient Descent SANN",{
parameters <- TestGradientDescentMethod(X = X, Y = Y, m = "SANN")
matrix <- ConfusionMatrix(X = X, Y = Y, parameters, cutoff = 0.68)
print(matrix)
})
```

### Conclusion

The most effective method is the BFGS since it produces fewer cases of error (greatest f1 score).


## Optional (+0.5 - 1 points in final grade). 

    + Implement the algorithm step by step using the update rule and an iterative algorithm, do not use the *optim* function.
    

```{r}
fastgrad <- function(X,Y,alpha,iter=1200){

# Reformat the data by adding a row of 1's for the constant.
  X = cbind(rep(1,nrow(X)),X)
  
# Number of rows X. 
  n = nrow(X)
  
# Theta is a matrix of 0s with 3 rows and 1 column.
  theta <- matrix(rep(0, ncol(X)), nrow = ncol(X), ncol = 1)
  
# Max # of iterations that gradient descent will run.
  thetaiter <- list(theta)
  
# Declare the vector of cost
  costiter <- c()

# Run 1200 iterations.
#In each iteration, a new set of parameters is calculated as follows: the previous set of parameters plus (learning speed) / number of rows * Transposed matrix of (Matrix product of the transposed matrix of (Matrix product of X and the last set of parameters (i-1) - Y) and X).
#The previous cost is compared with the current cost. When it is seen that the current one is greater than the previous one, it leaves the circuit.
  
for (i in 2:iter) {
  costiter[1] <- CostFunction(thetaiter[[1]], X, Y)
  thetaiter[[i]] <- thetaiter[[i - 1]] - (alpha/n)*t(t( X %*% thetaiter[[i - 1]] - Y) %*% X)
  costiter[i] <- CostFunction(thetaiter[[i]], X, Y)
  if (costiter[i] >= costiter[i - 1]) {
    break
  }
}
  
iteration <- 1:length(costiter)
#The final parameters set is the last one.
finaltheta <- thetaiter[[length(thetaiter)]]

return(finaltheta)
return(costiter)
return(iteration)
}
```    

Run the function to get the parameters and their cost.

```{r}
# run for alpha = 0.001
result1 <- fastgrad(X, Y, 0.001,1200)
print(paste("Final Parameters value: ", result1[3:4], sep = ""))
```

    + Research about regularization in logistic regression and explain it. 

*Regularization* is extremely important in logistic regression modeling. Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in high dimensions. Consequently, most logistic regression models use one of the following two strategies to dampen model complexity:

L2 regularization.
Early stopping, that is, limiting the number of training steps or the learning rate (@logisticregressionwithregularization).


---
references:
- id: rdocumentation
  title: General-purpose Optimization
  author:
  - family: R documentation
  URL: 'https://www.rdocumentation.org/packages/stats/versions/3.6.1/topics/optim'
  issued:
    year: 2019
    month: 3

- id: convexoptimizationinr
  title: Convex Optimization in R
  author:
  - family: Machine Learning Mastery
  URL: 'https://machinelearningmastery.com/convex-optimization-in-r/'
  issued:
    year: 2019
    
- id: logisticregressionwithregularization
  title: Logistic Regression: Loss and Regularization
  author:
  - family: Machine Learning courses
  URL: 'https://developers.google.com/machine-learning/crash-course/logistic-regression/model-training'
  issued:
    year: 2019
--- 
